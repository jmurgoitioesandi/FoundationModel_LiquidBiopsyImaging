{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from SimCLR import SimCLR\n",
    "from datasets import loading_datasets\n",
    "\n",
    "# Import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial17\"\n",
    "# In this notebook, we use data loaders with heavier computational processing. It is recommended to use as many\n",
    "# workers as possible in a data loader, which corresponds to the number of CPU cores\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "## Path to the file with the rare tiles\n",
    "RAREST_PATH = 'rare_tiles.npy'\n",
    "## Path to the slides\n",
    "SLIDES_PATH = '/scratch1/murgoiti/latestageBC_slides'\n",
    "## Load list of slides to consider\n",
    "with open(f'slide_list.txt') as f:\n",
    "    SLIDE_LIST = f.read().split('\\n')\n",
    "\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Number of workers:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simclr(batch_size, max_epochs=500, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, 'SimCLR'),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc_top5'),\n",
    "                                    LearningRateMonitor('epoch')])\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, 'SimCLR.ckpt')\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f'Found pretrained model at {pretrained_filename}, loading...')\n",
    "        model = SimCLR.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        dataset_normal, dataset_rare, dataset_normal_val, dataset_rare_val = loading_datasets(RAREST_PATH, SLIDES_PATH, SLIDE_LIST)\n",
    "\n",
    "        train_loader_rare = data.DataLoader(dataset_rare, batch_size=batch_size, shuffle=True,\n",
    "                                       drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "        train_loader_normal = data.DataLoader(dataset_normal, batch_size=batch_size, shuffle=True,\n",
    "                                       drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "        val_loader_rare = data.DataLoader(dataset_rare_val, batch_size=batch_size, shuffle=False,\n",
    "                                     drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "        val_loader_normal = data.DataLoader(dataset_normal_val, batch_size=batch_size, shuffle=False,\n",
    "                                     drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "        train_loader = zip(train_loader_rare, train_loader_normal)\n",
    "        val_loader = zip(val_loader_rare, val_loader_normal)\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = SimCLR(max_epochs=max_epochs, **kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = SimCLR.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr_model = train_simclr(batch_size=125,\n",
    "                            hidden_dim=128,\n",
    "                            lr=5e-4,\n",
    "                            temperature=0.07,\n",
    "                            weight_decay=1e-4,\n",
    "                            max_epochs=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
